# -*- coding: utf-8 -*-
"""““Untitled2.ipynb”的副本”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XOYrl7GzMmdbPqdddhxxqL0VORcsPxOg
"""

import os
import datetime
 
import IPython
import IPython.display
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder

csv_path = tf.keras.utils.get_file(
    origin='https://seafile.h.t123yh.xyz:3/f/515bf6f531a345a7b72f/?dl=1',
    fname='train.csv')


official_test_path = tf.keras.utils.get_file(
    origin='https://seafile.h.t123yh.xyz:3/f/7f10faed808140188440/?dl=1',
    fname='test.csv')

df = pd.read_csv(csv_path)
official_test = pd.read_csv(official_test_path)

batch_num = 1

OUT_FILE_PATH = 'raw_data/out_train.csv'

train_grouped = df.groupby("QUEUE_ID")
df["BATCH_NUM"] = None

def isNaN(num):
    return num != num

for name1, data in train_grouped:
    tmp = None
    for name, row in data.iterrows():
        if(tmp is None):
          tmp = row
        elif(isNaN(row["RESOURCE_TYPE"])):
          continue
        elif(row["DOTTING_TIME"] - tmp["DOTTING_TIME"] < 10000):
          merge_list = ['CPU_USAGE', 'MEM_USAGE','LAUNCHING_JOB_NUMS','RUNNING_JOB_NUMS','SUCCEED_JOB_NUMS','CANCELLED_JOB_NUMS','FAILED_JOB_NUMS']
          for dat in merge_list:
            t = max(row[dat],tmp[dat])
            df.at[name,dat] = t
          df.at[name-1,'BATCH_NUM'] = None
        elif(row["DOTTING_TIME"] - tmp["DOTTING_TIME"] > 320000):
          batch_num = batch_num+1
        df.at[name,'BATCH_NUM'] = batch_num
        tmp = row
    batch_num = batch_num+1

df.drop(df[df['BATCH_NUM'] != df['BATCH_NUM']].index, inplace = True) 
print(batch_num)

dftmp = df.groupby('BATCH_NUM')
dfnew = pd.DataFrame(columns=df.columns)


for name, data in dftmp:
  if len(data) >= 10:
    print(data)
'''
df.to_csv('./out.csv')
df.head()
'''
dftmp

import os
print(os.getcwd())

print(csv_path)

ddd = train_grouped.get_group(21673)
plot_cols = ['DISK_USAGE', 'CPU_ABS_USAGE', 'LAUNCHING_JOB_NUMS', 'SUCCEED_JOB_NUMS', 'DAY_SIN']
plot_features = ddd[plot_cols][:3000]
# plot_features.index = ddd['DOTTING_TIME'][:3000]
_ = plot_features.plot(subplots=True)

fft = tf.signal.rfft(ddd['CPU_ABS_USAGE'])
f_per_dataset = np.arange(0, len(fft))

n_samples_h = len(ddd['CPU_ABS_USAGE'])
hours_per_year = 288
years_per_dataset = n_samples_h/(hours_per_year)

f_per_year = f_per_dataset/years_per_dataset
plt.step(f_per_year, np.abs(fft))
plt.xscale('log')
plt.ylim(0, 100)
plt.xlim([0.1, 50])
plt.xticks([4.3], labels=['1/day'])
_ = plt.xlabel('Frequency (log scale)')

group_count = len(train_grouped)
test_group_count = 2
ntrain_groups = pd.concat([train_grouped.get_group(x) for x in list(train_grouped.groups.keys())[0:group_count - test_group_count]])
ntest_groups = pd.concat([train_grouped.get_group(x) for x in list(train_grouped.groups.keys())[group_count - test_group_count:]])

train_mean = train_groups.mean()
train_std = train_groups.std()

ntrain_groups

ddd.head(288)